#!/usr/bin/env python3

"""
Script to extract log and/or UHDM DBs out from artifacts generated by CI builds.
NOTE: CI artifacts are hierarhical tarballs i.e. tarball within tarball within tarball.
"""

import argparse
import multiprocessing
import os
import pprint
import re
import shutil
import sys
import tarfile
import time
import traceback
import zipfile

from datetime import datetime
from enum import Enum, unique
from pathlib import Path
from threading import Lock

from utils import (
  build_filters,
  is_windows,
  log,
  mkdir,
  rmdir,
  Status,
)

_this_filepath = Path(__file__).resolve()
_default_workspace_dirpath = _this_filepath.parent.parent

_default_dbname = 'surelog.uhdm'
_platform_ids = ['', '.linux', '.osx', '.win']
_default_test_dirpaths = [ _default_workspace_dirpath / 'tests', _default_workspace_dirpath / 'third_party' / 'tests' ]


def _scan(dirpaths: list[Path], filters: list[str]):
  def _is_filtered(name):
    if not filters:
      return True
    for filter in filters:
      if isinstance(filter, str):
        if filter.lower() == name.lower():
          return True
      elif filter.search(name):  # Note: match() reports success only if the match is at index 0
        return True
    return False

  tests = {}
  for dirpath in dirpaths:
    for filepath in dirpath.resolve().rglob('*.sl'):
      if _is_filtered(filepath.stem):
        tests[filepath.stem] = filepath

  return { name : tests[name] for name in sorted(tests.keys(), key=lambda t: t.lower()) }


def _extract_worker(params):
  zip_filepath, archive_name, modes, output_dirpath, queue = params

  result = 0
  with zipfile.ZipFile(zip_filepath, 'r') as zipfile_strm:
    with zipfile_strm.open(f'{archive_name}.tar.gz') as tarfile_strm:
      with tarfile.open(fileobj=tarfile_strm) as archive_strm:
        archive_names = set(archive_strm.getnames())

        while not queue.empty():
          try:
            params = queue.get(block=False)
          except queue.Empty:
            break

          if not params:
            break

          name, filepath = params
          test_filepath = f'{archive_name}/{name}.tar.gz'

          if test_filepath not in archive_names:
            continue

          dst_dirpath = output_dirpath / name if output_dirpath else filepath.parent
          mkdir(dst_dirpath)

          with tarfile.open(fileobj=archive_strm.extractfile(test_filepath)) as test_strm:
            if 'db' in modes:
              for slpp in ['slpp_all', 'slpp_unit']:
                src_db_filepath = f'{name}/{slpp}/{_default_dbname}'

                if src_db_filepath in test_strm.getnames():
                  dst_db_dirpath = dst_dirpath / slpp
                  mkdir(dst_db_dirpath)

                  dst_db_filepath = dst_db_dirpath / _default_dbname

                  try:
                    src_strm = test_strm.extractfile(src_db_filepath)

                    with open(dst_db_filepath, 'wb') as dst_strm:
                      dst_strm.write(src_strm.read())
                      dst_strm.flush()

                  except Exception:
                    log(f'Failed to extract \"{src_db_filepath}\"')
                    traceback.print_exc()
                    result += 1

                  break

            if 'log' in modes:
              for platform_id in _platform_ids:
                src_log_filepath = f'{name}/{name}{platform_id}.log'
                if src_log_filepath in test_strm.getnames():
                  dst_log_filepath = dst_dirpath / f'{name}{platform_id}.log'

                  log(f'{src_log_filepath} => {dst_log_filepath}')

                  try:
                    src_log_strm = test_strm.extractfile(src_log_filepath)

                    with dst_log_filepath.open('wb') as dst_log_strm:
                      dst_log_strm.write(src_log_strm.read())
                      dst_log_strm.flush()

                    # On Windows, fixup the line endings
                    if is_windows():
                      with dst_log_filepath.open() as istrm:
                        lines = istrm.readlines()
                      with dst_log_filepath.open('wt') as ostrm:
                        ostrm.writelines(lines)
                        ostrm.flush()

                  except KeyError:
                    log(f'Failed to extract \"{src_log_filepath}\"')
                    traceback.print_exc()
                    result += 1

  return result


def _extract(args, tests):
  if not tests:
    return 0  # No selected tests

  if not args.zip_filepath:
    raise "Missing required archive path"

  if not args.zip_filepath.is_file():
    raise "Input archive path is invalid"

  archive_name = args.zip_filepath.name
  pos = str(archive_name).find('.')
  if pos >= 0:
    archive_name = str(archive_name)[:pos]

  manager = multiprocessing.Manager()
  queue = manager.Queue()
  for name, filepath in tests.items():
    queue.put((name, filepath))

  if args.jobs <= 1:
    results = [_extract_worker((args.zip_filepath, archive_name, args.modes, args.output_dirpath, queue))]
  else:
    with multiprocessing.Pool(processes=args.jobs) as pool:
      results = pool.map(
        _extract_worker,
        [(args.zip_filepath, archive_name, args.modes, args.output_dirpath, queue)] * args.jobs
      )

  return sum(results)


def _main():
  start_dt = datetime.now()
  print(f'Starting CI artifact extraction @ {str(start_dt)}')

  parser = argparse.ArgumentParser()
  parser.add_argument('modes', nargs='+', choices=['db', 'log'], type=str, help='Pick what to extract')
  parser.add_argument(
      '--output-dirpath', dest='output_dirpath', required=False, type=str,
      help='Output directory path to extract to. If provided, all files are extracted with input directory as root.')
  parser.add_argument(
      '--filters', nargs='+', required=False, default=[], type=str, help='Filter tests matching these regex inputs')
  parser.add_argument(
      '--jobs', nargs='?', required=False, default=multiprocessing.cpu_count(), type=int,
      help='Run tests in parallel, optionally providing max number of concurrent processes. Set 0 to run sequentially.')
  parser.add_argument(
      '--zip-filepath', dest='zip_filepath', required=True, type=str, help='Path to zipfile to extract logs from.')
  args = parser.parse_args()

  args.modes = sorted(set(args.modes))
  args.zip_filepath = Path(args.zip_filepath).resolve()

  if args.output_dirpath:
    args.output_dirpath = Path(args.output_dirpath).resolve()
    mkdir(args.output_dirpath)

  args.filters = build_filters(args.filters)
  tests = _scan(_default_test_dirpaths, args.filters)

  if (args.jobs == None) or (args.jobs > multiprocessing.cpu_count()):
    args.jobs = multiprocessing.cpu_count()
  if args.jobs > len(tests):
    args.jobs = len(tests)

  print( 'Environment:')
  print(f'   command-line: {" ".join(sys.argv)}')
  print(f'current-dirpath: {os.getcwd()}')
  print(f'   zip-filepath: {args.zip_filepath}')
  print(f' output-dirpath: {args.output_dirpath}')
  print(f'        filters: {args.filters}')
  print(f'           jobs: {args.jobs}')
  print(f'          tests: {len(tests)}')
  print( '')

  print(f'Extracting {args.zip_filepath} ...')
  result = _extract(args, tests)

  end_dt = datetime.now()
  delta = round((end_dt - start_dt).total_seconds())

  print( '')
  print(f'Extraction completed @ {str(end_dt)} in {str(delta)} seconds, with {result} failures.')
  return result


if __name__ == '__main__':
  sys.exit(_main())
